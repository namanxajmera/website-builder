This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.cursor/
  environment.json
.gitignore
crawl_site.py
dashboard.py
README.md
remake_site_with_ai.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(git rm:*)",
      "Bash(rm:*)",
      "Bash(rg:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".cursor/environment.json">
{
  "snapshot": "snapshot-20250520-757947b1-3a90-4cc5-93ad-deffe0aaa42c",
  "terminals": []
}
</file>

<file path="crawl_site.py">
import sys
import os
import re
from urllib.parse import urljoin, urlparse, quote
from termcolor import cprint
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import argparse
import hashlib
import json
import datetime
import socket
import ipaddress

# Default values, can be overridden by CLI args
DEFAULT_MAX_PAGES = 20
DEFAULT_CRAWL_DEPTH = 2

# File name constants (consistent with remake_site_with_ai.py)
COPY_FILENAME = "copy.txt"
CSS_FILENAME = "css.txt"
HTML_FILENAME = "page.html"
IMAGES_FILENAME = "images.txt"
URL_FILENAME = "url.txt"

def is_safe_url(url):
    """Validate URL to prevent SSRF attacks by checking if target IP is public"""
    try:
        parsed = urlparse(url)
        hostname = parsed.hostname
        
        if not hostname:
            cprint(f"[ERROR] Invalid URL: no hostname found in {url}", "red")
            return False
            
        # Resolve hostname to IP
        try:
            ip_str = socket.gethostbyname(hostname)
            ip_obj = ipaddress.ip_address(ip_str)
            
            # Check if IP is in private/reserved ranges
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_reserved:
                cprint(f"[ERROR] SSRF risk detected: {url} resolves to private/reserved IP {ip_str}", "red")
                return False
                
            # Additional checks for common internal ranges
            if (ip_obj.is_link_local or 
                str(ip_obj).startswith('169.254.') or  # Link-local
                str(ip_obj).startswith('224.') or      # Multicast
                str(ip_obj) == '0.0.0.0'):            # Unspecified
                cprint(f"[ERROR] SSRF risk detected: {url} resolves to restricted IP {ip_str}", "red")
                return False
                
            cprint(f"[INFO] URL safety check passed: {hostname} -> {ip_str}", "green")
            return True
            
        except socket.gaierror as e:
            cprint(f"[ERROR] DNS resolution failed for {hostname}: {e}", "red")
            return False
            
    except Exception as e:
        cprint(f"[ERROR] URL safety validation failed for {url}: {e}", "red")
        return False

def setup_driver():
    cprint("[INFO] Setting up Chrome WebDriver...", "cyan")
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')  # Added for stability
        driver = webdriver.Chrome(options=chrome_options)
        # Set timeouts to prevent hanging
        driver.set_page_load_timeout(30)  # 30 seconds timeout for page loading
        driver.implicitly_wait(10)  # 10 seconds for element finding
        cprint("[SUCCESS] Chrome WebDriver initialized successfully with 30s page load timeout", "green")
        return driver
    except Exception as e:
        cprint(f"[ERROR] Failed to initialize Chrome WebDriver: {e}", "red")
        cprint("[INFO] Please ensure ChromeDriver is installed and in PATH", "yellow")
        sys.exit(1)

def is_internal_link(href, base_netloc):
    if not href or href.startswith("mailto:") or href.startswith("tel:"):
        return False
    parsed = urlparse(href)
    # Schemes like 'javascript:' should be ignored
    if parsed.scheme and parsed.scheme not in ['http', 'https']:
        return False
    if parsed.netloc and parsed.netloc != base_netloc:
        return False
    # We will handle fragment checks specifically in the crawl loop to avoid redundant crawls
    return True

def get_page_content(driver, url):
    cprint(f"[INFO] Loading page content from: {url}", "cyan")
    
    # SSRF protection: validate URL safety before making request
    if not is_safe_url(url):
        cprint(f"[ERROR] URL blocked for security reasons: {url}", "red")
        return None, None, None, None
    
    try:
        driver.get(url)
        cprint(f"[SUCCESS] Page loaded successfully", "green")
    except TimeoutException:
        cprint(f"[ERROR] Page load timeout for {url} (exceeded 30 seconds)", "red")
        return None, None, None, None
    except Exception as e:
        cprint(f"[ERROR] Failed to load page {url}: {e}", "red")
        return None, None, None, None
    
    try:
        
        cprint("[INFO] Parsing HTML content...", "cyan")
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        
        cprint("[INFO] Extracting images...", "cyan")
        images = set()
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                # Resolve relative URLs and add to set
                images.add(urljoin(url, src.strip()))
        cprint(f"[SUCCESS] Found {len(images)} images", "green")
        
        cprint("[INFO] Extracting CSS files...", "cyan")
        css_files = set()
        for link_tag in soup.find_all('link', rel='stylesheet'):
            href = link_tag.get('href')
            if href:
                css_files.add(urljoin(url, href.strip()))
        cprint(f"[SUCCESS] Found {len(css_files)} CSS files", "green")
        
        cprint("[INFO] Extracting inline styles...", "cyan")
        inline_styles = []
        for style_tag in soup.find_all('style'):
            if style_tag.string:
                inline_styles.append(style_tag.string.strip())
        cprint(f"[SUCCESS] Found {len(inline_styles)} inline style blocks", "green")
        
        cprint("[INFO] Extracting text content...", "cyan")
        for script_or_style in soup(["script", "style"]): # Remove script and style tags before extracting text
            script_or_style.decompose()
        text = soup.get_text(separator='\n', strip=True)
        text_length = len(text.split())
        cprint(f"[SUCCESS] Extracted {text_length} words of text content", "green")
        
        return html, images, text, css_files, inline_styles
    except Exception as e:
        cprint(f"[ERROR] Failed to load or parse {url}: {e}", "red")
        return None, set(), "", set(), []

def get_site_output_dir(domain):
    cprint(f"[INFO] Creating output directory for domain: {domain}", "cyan")
    base_dir = domain.replace(':', '_').replace('.', '_') # Sanitize for port numbers and dots
    output_dir = base_dir
    count = 1
    # Ensure a unique directory name
    while os.path.exists(output_dir):
        output_dir = f"{base_dir}_{count}"
        count += 1
    
    try:
        os.makedirs(output_dir)
        cprint(f"[SUCCESS] Created output directory: {output_dir}", "green")
        return output_dir
    except Exception as e:
        cprint(f"[ERROR] Failed to create output directory {output_dir}: {e}", "red")
        sys.exit(1)

def url_to_folder_name(url_str, base_url_str):
    cprint(f"[INFO] Converting URL to folder name: {url_str}", "cyan")
    parsed_url = urlparse(url_str)
    
    # Normalize by removing fragment and trailing slash for comparison and path generation
    normalized_url_path = parsed_url.path.rstrip('/')
    normalized_base_url_path = urlparse(base_url_str).path.rstrip('/')

    if normalized_url_path == normalized_base_url_path or normalized_url_path == "":
        folder_name = "home"
    else:
        path_segments = [seg for seg in normalized_url_path.split('/') if seg]
        if not path_segments: # Should be caught by home, but as a fallback
            folder_name = "root_page"
        else:
            # Take the last significant part of the path, or join them
            folder_name = path_segments[-1] 
            # If it ends with a common extension, remove it
            common_extensions = ['.html', '.htm', '.php', '.asp', '.aspx']
            for ext in common_extensions:
                if folder_name.endswith(ext):
                    folder_name = folder_name[:-len(ext)]
                    break
        
        # Sanitize folder name
        folder_name = re.sub(r'[^\w\-_]', '_', folder_name) # Allow word chars, hyphen, underscore
        folder_name = re.sub(r'_+', '_', folder_name)      # Collapse multiple underscores
        folder_name = folder_name.strip('_')               # Remove leading/trailing underscores
        
        if not folder_name: # If sanitization results in empty, default
            folder_name = "page_" + hashlib.md5(url_str.encode()).hexdigest()[:8]

    # Handle query parameters by appending a sanitized hash if they exist
    if parsed_url.query:
        query_hash = hashlib.md5(parsed_url.query.encode()).hexdigest()[:6]
        folder_name = f"{folder_name}_q{query_hash}"

    cprint(f"[SUCCESS] Generated folder name: {folder_name}", "green")
    return folder_name

def save_page_data(site_dir, folder_name, url, html, text, images, css_files, inline_styles):
    cprint(f"[INFO] Saving page data to folder: {folder_name}", "cyan")
    
    page_dir = os.path.join(site_dir, folder_name)
    try:
        os.makedirs(page_dir, exist_ok=True)
    except Exception as e:
        cprint(f"[ERROR] Failed to create page directory {page_dir}: {e}", "red")
        return False

    try:
        # Save URL
        with open(os.path.join(page_dir, URL_FILENAME), "w", encoding="utf-8") as f:
            f.write(url)
        cprint(f"  ‚úì Saved {URL_FILENAME}", "green")

        # Save HTML
        with open(os.path.join(page_dir, HTML_FILENAME), "w", encoding="utf-8") as f:
            f.write(html)
        cprint(f"  ‚úì Saved {HTML_FILENAME}", "green")

        # Save text content
        with open(os.path.join(page_dir, COPY_FILENAME), "w", encoding="utf-8") as f:
            f.write(text)
        cprint(f"  ‚úì Saved {COPY_FILENAME}", "green")

        # Save images
        with open(os.path.join(page_dir, IMAGES_FILENAME), "w", encoding="utf-8") as f:
            for img_url in images:
                f.write(img_url + "\n")
        cprint(f"  ‚úì Saved {IMAGES_FILENAME} with {len(images)} image URLs", "green")

        # Save CSS (external + inline)
        css_output_content = ""
        if css_files:
            css_output_content += "/* --- External CSS Files --- */\n"
            for css_url in css_files:
                css_output_content += f"/* Original URL: {css_url} */\n\n"
        if inline_styles:
            css_output_content += "\n/* --- Inline Styles --- */\n"
            for style_block in inline_styles:
                 css_output_content += f"<style>\n{style_block}\n</style>\n\n"
            
        with open(os.path.join(page_dir, CSS_FILENAME), "w", encoding="utf-8") as f:
            f.write(css_output_content)
        cprint(f"  ‚úì Saved {CSS_FILENAME} ({len(css_files)} external CSS refs, {len(inline_styles)} inline styles)", "green")

        return True
    except Exception as e:
        cprint(f"[ERROR] Failed to save page data files in {page_dir}: {e}", "red")
        return False

def main():
    parser = argparse.ArgumentParser(description="Crawl a website and extract content for AI processing.")
    parser.add_argument("url", help="The website URL to crawl")
    parser.add_argument("--max_pages", type=int, default=DEFAULT_MAX_PAGES,
                        help=f"Maximum number of pages to crawl (default: {DEFAULT_MAX_PAGES})")
    parser.add_argument("--depth", type=int, default=DEFAULT_CRAWL_DEPTH,
                        help=f"Maximum crawl depth (default: {DEFAULT_CRAWL_DEPTH})")
    
    args = parser.parse_args()

    target_url_raw = args.url
    # Normalize the initial target URL (remove fragment, trailing slash)
    parsed_target_url_raw = urlparse(target_url_raw)
    target_url = parsed_target_url_raw._replace(fragment="", query="").geturl().rstrip('/')
    if not target_url: # If only a fragment was passed
        target_url = parsed_target_url_raw._replace(path="/", fragment="", query="").geturl().rstrip('/')

    max_pages = args.max_pages
    crawl_depth = args.depth

    cprint("="*60, "cyan")
    cprint(f"üï∑Ô∏è  WEBSITE CRAWLER STARTING", "cyan", attrs=["bold"])
    cprint("="*60, "cyan")
    cprint(f"[INFO] Target URL (normalized): {target_url}", "cyan")
    cprint(f"[INFO] Max pages: {max_pages}", "cyan")
    cprint(f"[INFO] Crawl depth: {crawl_depth}", "cyan")
    cprint("="*60, "cyan")

    try:
        parsed_url_obj = urlparse(target_url)
        if not parsed_url_obj.scheme or not parsed_url_obj.netloc:
            raise ValueError("Invalid URL format after normalization")
        base_netloc = parsed_url_obj.netloc
        cprint(f"[SUCCESS] Parsed base domain: {base_netloc}", "green")
    except Exception as e:
        cprint(f"[ERROR] Invalid URL '{target_url_raw}': {e}", "red")
        sys.exit(1)

    site_dir = get_site_output_dir(base_netloc)
    driver = setup_driver()

    try:
        visited_path_query = set() # Store URL path+query to avoid re-crawling same content due to fragments
        to_visit_queue = [(target_url, 1)]  # (url_with_fragment, depth)
        pages_crawled_count = 0
        
        # Keep track of unique URLs added to queue to avoid duplicates in queue
        queued_path_query_depth = set() 
        queued_path_query_depth.add((target_url, 1))

        cprint("\n" + "="*60, "cyan")
        cprint("üöÄ STARTING CRAWL PROCESS", "cyan", attrs=["bold"])
        cprint("="*60, "cyan")

        while to_visit_queue and pages_crawled_count < max_pages:
            current_url_full, current_depth = to_visit_queue.pop(0)
            
            parsed_current_full = urlparse(current_url_full)
            # Key for uniqueness: URL without fragment
            url_path_query_key = parsed_current_full._replace(fragment="").geturl()

            if url_path_query_key in visited_path_query:
                cprint(f"[SKIP] Content for {url_path_query_key} (from {current_url_full}) already processed.", "yellow")
                continue
            
            if current_depth > crawl_depth:
                cprint(f"[SKIP] {current_url_full} - Max depth ({crawl_depth}) reached.", "yellow")
                continue

            cprint(f"\n[INFO] Crawling (depth {current_depth}, page {pages_crawled_count + 1}/{max_pages}): {current_url_full}", "cyan", attrs=["bold"])

            html, images, text, css_files, inline_styles = get_page_content(driver, url_path_query_key) # Crawl URL without fragment
            
            if html:
                visited_path_query.add(url_path_query_key)
                pages_crawled_count += 1
                
                folder_name_for_this_instance = url_to_folder_name(current_url_full, target_url)
                
                if save_page_data(site_dir, folder_name_for_this_instance, current_url_full, html, text, images, css_files, inline_styles):
                    cprint(f"[SUCCESS] Saved data for {current_url_full} (content from {url_path_query_key}) in {site_dir}/{folder_name_for_this_instance}", "green", attrs=["bold"])
                else:
                    cprint(f"[ERROR] Failed to save data for {current_url_full}", "red")

                if current_depth < crawl_depth:
                    cprint(f"[INFO] Searching for internal links on {url_path_query_key} (depth: {current_depth}/{crawl_depth})", "cyan")
                    soup = BeautifulSoup(html, "html.parser")
                    new_links_added_to_queue = 0
                    for link_tag in soup.find_all('a', href=True):
                        href_attr = link_tag['href']
                        absolute_link = urljoin(url_path_query_key, href_attr)
                        parsed_absolute_link = urlparse(absolute_link)
                        
                        # Key for queue uniqueness check (URL without fragment)
                        link_path_query_key_for_queue = parsed_absolute_link._replace(fragment="").geturl()

                        if is_internal_link(absolute_link, base_netloc) and \
                           link_path_query_key_for_queue not in visited_path_query and \
                           (link_path_query_key_for_queue, current_depth + 1) not in queued_path_query_depth:
                            
                            to_visit_queue.append((absolute_link, current_depth + 1))
                            queued_path_query_depth.add((link_path_query_key_for_queue, current_depth + 1))
                            new_links_added_to_queue += 1
                    
                    if new_links_added_to_queue > 0:
                        cprint(f"[SUCCESS] Added {new_links_added_to_queue} new unique internal links to queue", "green")
                    else:
                        cprint("[INFO] No new unique internal links found to add to queue", "yellow")
            else:
                cprint(f"[ERROR] Failed to get content for {url_path_query_key} (from {current_url_full})", "red")

        cprint("\n" + "="*60, "green")
        cprint("‚úÖ CRAWL COMPLETED SUCCESSFULLY", "green", attrs=["bold"])
        cprint("="*60, "green")
        cprint(f"[DONE] Processed {pages_crawled_count} unique pages. Data saved in '{site_dir}'", "green", attrs=["bold"])
        cprint("="*60, "green")
        
        # Generate manifest file for reliable inter-script communication
        manifest_data = {
            "version": "1.0",
            "timestamp": datetime.datetime.now().isoformat(),
            "crawl_info": {
                "target_url": target_url,
                "base_netloc": base_netloc,
                "max_pages": max_pages,
                "crawl_depth": crawl_depth,
                "pages_crawled": pages_crawled_count
            },
            "output": {
                "site_dir": site_dir,
                "crawled_pages": list(visited_path_query)
            },
            "status": "completed"
        }
        
        manifest_path = os.path.join(site_dir, "crawl_manifest.json")
        try:
            with open(manifest_path, "w", encoding="utf-8") as f:
                json.dump(manifest_data, f, indent=2)
            cprint(f"[SUCCESS] Manifest saved to {manifest_path}", "green")
        except Exception as e:
            cprint(f"[ERROR] Failed to save manifest: {e}", "red")
        
        print(site_dir) # For dashboard parsing (kept for backward compatibility)

    except KeyboardInterrupt:
        cprint("\n[WARN] Crawl interrupted by user", "yellow")
        cprint(f"[INFO] Partial data saved in '{site_dir}' ({pages_crawled_count} pages)", "cyan")
    except Exception as e:
        cprint(f"\n[ERROR] Unexpected error during crawling: {e}", "red")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        if 'driver' in locals() and driver:
            cprint("[INFO] Closing WebDriver...", "cyan")
            driver.quit()
            cprint("[SUCCESS] WebDriver closed", "green")

if __name__ == "__main__":
    main()
</file>

<file path="requirements.txt">
requests
termcolor
selenium 
beautifulsoup4
google-genai
streamlit
watchdog
</file>

<file path=".gitignore">
# Python virtual environment
venv/

# Byte-compiled / cache files
__pycache__/
*.py[cod]

# Environment variables
.env.local
.env

# Project output directories
output/
*_ai/
*_crawled/

# Scraped site folders (common patterns)
# Domain-like folders
*.com/
*.org/
*.net/
*.io/
*.co/
*_com/
*_org/
*_net/
*_io/
*_co/

# Development and testing sites
localhost_*/
127_0_0_1_*/
example_com/
test_*/
demo_*/

# OS files
.DS_Store

# Jupyter Notebook checkpoints
.ipynb_checkpoints/

# Always include .cursor folder
!.cursor/
</file>

<file path="dashboard.py">
import streamlit as st
import subprocess
import os
import sys
from pathlib import Path
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
import threading
import queue
import json

# --- Page Configuration ---
st.set_page_config(
    page_title="AI Website Modernizer",
    page_icon="‚ú®",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- Session State Initialization ---
# (Ensures that these variables persist across reruns within a user's session)
if 'current_preview_file' not in st.session_state:
    st.session_state.current_preview_file = None
if 'ai_output_folder' not in st.session_state:
    st.session_state.ai_output_folder = None
if 'log_text' not in st.session_state:
    st.session_state.log_text = ""
if 'process_running' not in st.session_state:
    st.session_state.process_running = False
if 'step_status' not in st.session_state: # Stores status like "pending", "running", "completed", "error"
    st.session_state.step_status = {}
if 'start_process_url' not in st.session_state:
    st.session_state.start_process_url = None
if 'realtime_logs' not in st.session_state:
    st.session_state.realtime_logs = ""
if 'current_step' not in st.session_state:
    st.session_state.current_step = None

# --- UI Styling and Constants ---
PROC_STEPS = {
    "validate": "URL Validation",
    "crawl": "Website Crawling",
    "ai": "AI Modernization",
    "generate": "File Generation"
}

# --- Logging and Status Update Functions ---
def append_log_to_ui(raw_message_from_subprocess):
    st.session_state.log_text += raw_message_from_subprocess
    # In a real-time scenario with long logs, you might need to manage log length
    # or use a more sophisticated logging component if Streamlit struggles.

def clear_ui_logs_and_state():
    st.session_state.log_text = ""
    st.session_state.step_status = {}
    st.session_state.current_preview_file = None
    st.session_state.ai_output_folder = None

def read_crawl_manifest(potential_site_folders):
    """Read crawl manifest to get site directory instead of parsing stdout"""
    for folder in potential_site_folders:
        if not os.path.isdir(folder):
            continue
        manifest_path = os.path.join(folder, "crawl_manifest.json")
        if os.path.exists(manifest_path):
            try:
                with open(manifest_path, "r", encoding="utf-8") as f:
                    manifest = json.load(f)
                if manifest.get("status") == "completed":
                    return manifest["output"]["site_dir"], manifest
            except Exception as e:
                print(f"[DASHBOARD_ERROR] Failed to read manifest {manifest_path}: {e}")
    return None, None

def update_step_status(step_key, status):
    st.session_state.step_status[step_key] = status

# --- HTML Preview Modification ---
def modify_html_for_preview(html_content, current_filename, ai_output_folder_path_str):
    ai_output_folder_path = Path(ai_output_folder_path_str)
    soup = BeautifulSoup(html_content, "html.parser")
    
    global_css_path = ai_output_folder_path / "global_styles.css"
    if global_css_path.exists():
        with open(global_css_path, "r", encoding="utf-8") as f_css:
            css_content = f_css.read()
        for link_tag in soup.find_all('link', rel='stylesheet'):
            if 'global_styles.css' in link_tag.get('href', ''):
                link_tag.decompose()
        style_tag = soup.new_tag('style')
        style_tag.string = css_content
        if soup.head: soup.head.append(style_tag)
        else: soup.insert(0, style_tag)

    available_html_files = [
        f.name for f in ai_output_folder_path.iterdir() 
        if f.is_file() and f.name.endswith('.html')
    ]
    
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        parsed_href = urlparse(href)
        if parsed_href.scheme or parsed_href.netloc: # External link
            a_tag['target'] = '_blank'
            continue
        if href.endswith('.html') and href in available_html_files: # Internal link to another generated page
            a_tag['href'] = f"/?preview_file={href}" # Use query params for Streamlit navigation
            a_tag['target'] = '_self' # Ensure navigation happens in the main window
    return str(soup)

def display_preview(filename_to_display, ai_folder_path_str):
    ai_folder_path = Path(ai_folder_path_str)
    if not filename_to_display or not ai_folder_path.exists():
        st.info("Select an HTML file to preview or run the process first.")
        return

    html_file_path = ai_folder_path / filename_to_display
    if html_file_path.exists() and html_file_path.suffix == '.html':
        print(f"[DASHBOARD_INFO] Displaying preview of: {html_file_path}")
        with open(html_file_path, "r", encoding="utf-8") as f:
            html_content = f.read()
        
        modified_html = modify_html_for_preview(html_content, filename_to_display, ai_folder_path_str)
        st.markdown("---")
        st.subheader(f"Preview of `{filename_to_display}`")
        st.markdown(
            "**Note:** Preview is experimental. Navigation between pages *within this preview* is enabled. "
            "External links open in a new tab. For the most accurate view, open the generated files "
            f"from the output folder (`{ai_folder_path.name}/`) in your browser."
        )
        st.components.v1.html(modified_html, height=600, scrolling=True)
    else:
        print(f"[DASHBOARD_WARN] Could not find HTML file {html_file_path} to display.")
        st.warning(f"Could not find HTML file `{filename_to_display}` in `{ai_folder_path_str}`.")

# --- Core Process Functions ---
def run_subprocess_and_log(command_array, step_key):
    step_name_for_ui = PROC_STEPS.get(step_key, step_key.capitalize())
    update_step_status(step_key, "running")
    st.session_state.current_step = step_key
    print(f"\n[DASHBOARD_RUNNING_STEP] {step_name_for_ui}") # For terminal log
    print(f"[DASHBOARD_COMMAND] {' '.join(command_array)}")
    
    process_output_for_ui_log = ""
    start_time = time.time()
    
    try:
        process = subprocess.Popen(command_array, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
        for line in process.stdout:
            print(line, end='') # Print to terminal in real-time
            process_output_for_ui_log += line
            # Update session state with partial logs for real-time display
            st.session_state.realtime_logs = process_output_for_ui_log
        process.wait()
        
        append_log_to_ui(process_output_for_ui_log) # Append all output at once

        elapsed_time = time.time() - start_time
        if process.returncode == 0:
            update_step_status(step_key, "completed")
            print(f"[DASHBOARD_STEP_SUCCESS] {step_name_for_ui} completed in {elapsed_time:.2f}s.")
            return process.returncode, process_output_for_ui_log
        else:
            update_step_status(step_key, "error")
            print(f"[DASHBOARD_STEP_ERROR] {step_name_for_ui} failed (code {process.returncode}) in {elapsed_time:.2f}s.")
            return process.returncode, process_output_for_ui_log
    except subprocess.TimeoutExpired:
        elapsed_time = time.time() - start_time
        timeout_msg = f"{step_name_for_ui} process timed out after {elapsed_time:.2f}s."
        print(f"[DASHBOARD_STEP_ERROR] {timeout_msg}")
        append_log_to_ui(timeout_msg + "\n")
        update_step_status(step_key, "error")
        return -1, timeout_msg
    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = f"Exception during {step_name_for_ui} ({elapsed_time:.2f}s): {str(e)}"
        print(f"[DASHBOARD_STEP_ERROR] {error_msg}")
        append_log_to_ui(error_msg + "\n")
        update_step_status(step_key, "error")
        return -2, error_msg

def run_full_process(target_url):
    st.session_state.process_running = True
    clear_ui_logs_and_state() # Resets logs and status for new run
    st.session_state.process_running = True # Set back to true after clear

    print(f"\n[DASHBOARD_INFO] Full process started for: {target_url}")
    append_log_to_ui(f"üöÄ Initializing modernization for: {target_url}\n")

    # Validate URL
    if not (target_url.startswith("http://") or target_url.startswith("https://")):
        msg = "Invalid URL: Must start with http:// or https://"
        update_step_status("validate", "error")
        st.error(msg); print(f"[DASHBOARD_ERROR] {msg}"); append_log_to_ui(msg + "\n")
        st.session_state.process_running = False; return
    match = re.search(r"https?://([^/]+)", target_url)
    if not match:
        msg = "Could not parse domain from URL."
        update_step_status("validate", "error")
        st.error(msg); print(f"[DASHBOARD_ERROR] {msg}"); append_log_to_ui(msg + "\n")
        st.session_state.process_running = False; return
    domain_base = match.group(1).replace(':', '_')
    update_step_status("validate", "completed")
    print(f"[DASHBOARD_INFO] Domain parsed: {domain_base}")

    # Crawl
    crawl_cmd = [sys.executable, "crawl_site.py", target_url]
    crawl_return_code, crawl_output = run_subprocess_and_log(crawl_cmd, "crawl")
    site_folder = None
    if crawl_return_code == 0:
        # Try manifest-based approach first (reliable)
        potential_folders = sorted([d for d in Path(".").iterdir() if d.is_dir() and d.name.startswith(domain_base.replace('.','_')) and not d.name.endswith("_ai")], key=lambda p: p.stat().st_mtime, reverse=True)
        site_folder, manifest = read_crawl_manifest(potential_folders)
        
        if site_folder:
            print(f"[DASHBOARD_INFO] Site folder found via manifest: {site_folder} (crawled {manifest['crawl_info']['pages_crawled']} pages)")
        else:
            # Legacy fallback: parse stdout (for backward compatibility)
            lines = crawl_output.strip().split('\n')
            if lines:
                last_line = lines[-1].strip()
                if os.path.isdir(last_line) and (domain_base.replace('.','_') in last_line or domain_base in last_line):
                    site_folder = last_line
                    print(f"[DASHBOARD_WARN] Using legacy stdout parsing for site folder: {site_folder}")
            
            # Final fallback: directory search
            if not site_folder and potential_folders:
                site_folder = str(potential_folders[0])
                print(f"[DASHBOARD_WARN] Using directory search fallback: {site_folder}")
        
        if not site_folder or not os.path.isdir(site_folder):
            msg = f"Crawl output folder not found for '{domain_base}'."
            update_step_status("crawl", "error"); st.error(msg); print(f"[DASHBOARD_ERROR] {msg}"); append_log_to_ui(msg + "\n")
            st.session_state.process_running = False; return
        print(f"[DASHBOARD_INFO] Crawled data in: {site_folder}")
    else:
        st.error("Crawl process failed. Check logs."); st.session_state.process_running = False; return

    # AI Remake
    remake_cmd = [sys.executable, "remake_site_with_ai.py", site_folder, 
                  "--model", st.session_state.get("ai_model", "gemini-2.5-flash-preview-05-20"),
                  "--temperature", str(st.session_state.get("ai_temperature", 0.5))]
    ai_return_code, _ = run_subprocess_and_log(remake_cmd, "ai")
    st.session_state.ai_output_folder = site_folder.rstrip('/').rstrip('\\') + "_ai"

    if ai_return_code == 0:
        update_step_status("generate", "completed")
        st.success(f"üéâ Modernization complete! Output in: {st.session_state.ai_output_folder}")
        print(f"[DASHBOARD_SUCCESS] Modernization complete! Output: {st.session_state.ai_output_folder}")
        
        # Display AI structural decision if available
        decision_file = os.path.join(st.session_state.ai_output_folder, "ai_decision.txt")
        if os.path.exists(decision_file):
            try:
                with open(decision_file, "r", encoding="utf-8") as f:
                    ai_decision = f.read().strip()
                if ai_decision:
                    st.info(f"üß† **AI Structural Decision:** {ai_decision}")
            except Exception as e:
                print(f"[DASHBOARD_WARN] Failed to read AI decision file: {e}")
        
        available_html_files = [f.name for f in Path(st.session_state.ai_output_folder).iterdir() if f.is_file() and f.name.endswith('.html')]
        if "index.html" in available_html_files: st.session_state.current_preview_file = "index.html"
        elif available_html_files: st.session_state.current_preview_file = available_html_files[0]
        else: st.warning("No HTML files found in AI output for preview.")
    else:
        st.error("AI modernization process failed. Check logs."); st.session_state.ai_output_folder = None
    
    st.session_state.process_running = False
    st.rerun()

# --- UI Layout ---

# Custom CSS for header/footer and enhanced styling
st.markdown("""
<style>
    .reportview-container .main .block-container {
        padding-top: 1rem;
        min-height: 100vh;
        display: flex;
        flex-direction: column;
    }
    .main-content {
        flex: 1;
    }
    .status-card {
        text-align: center;
        padding: 1rem;
        border: 2px solid #e0e0e0;
        border-radius: 0.5rem;
        background-color: #f9f9f9;
        margin: 0.25rem;
        transition: all 0.3s ease;
    }
    .status-card:hover {
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    .status-running {
        border-color: #ffc107;
        background-color: #fff3cd;
    }
    .status-completed {
        border-color: #28a745;
        background-color: #d4edda;
    }
    .status-error {
        border-color: #dc3545;
        background-color: #f8d7da;
    }
</style>
""", unsafe_allow_html=True)

# Main application content area
main_content_area = st.container()

with main_content_area:
    st.markdown('<div class="main-content">', unsafe_allow_html=True)
    
    # Input Section
    col_input, col_run, col_reset = st.columns([4, 2, 1])
    
    with col_input:
        url_input_val = st.text_input(
            "Target Website URL", 
            "https://example.com", 
            key="url_input_main",
            help="Enter the full URL (e.g., https://www.example.com) of the website you want to modernize.",
            label_visibility="collapsed"
        )
        
        # AI Configuration Options
        col_model, col_temp = st.columns([2, 1])
        with col_model:
            ai_model = st.selectbox(
                "AI Model",
                ["gemini-2.5-flash-preview-05-20", "gemini-1.5-pro-latest"],
                index=0,
                help="Select the Gemini model: Flash for speed, Pro for comprehensive analysis"
            )
        with col_temp:
            ai_temperature = st.slider(
                "Creativity",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.1,
                help="Higher values make AI output more creative and varied"
            )
    
    with col_run:
        if st.button("üöÄ Modernize Website!", type="primary", use_container_width=True, disabled=st.session_state.process_running):
            if url_input_val and (url_input_val.startswith("http://") or url_input_val.startswith("https://")):
                st.session_state.start_process_url = url_input_val
                st.session_state.ai_model = ai_model
                st.session_state.ai_temperature = ai_temperature
                st.session_state.process_running = True
                st.rerun()
            else:
                st.error("Please enter a valid website URL starting with http:// or https://")
    
    with col_reset:
        if st.button("üóëÔ∏è Reset", use_container_width=True, disabled=st.session_state.process_running):
            clear_ui_logs_and_state()
            st.rerun()

    # Check if a process should be started
    if st.session_state.start_process_url:
        url_to_process = st.session_state.start_process_url
        st.session_state.start_process_url = None  # Clear the flag
        run_full_process(url_to_process)

    # Preview Section
    if st.session_state.ai_output_folder and Path(st.session_state.ai_output_folder).exists():
        st.markdown("---")
        st.subheader("üñºÔ∏è Generated Site Preview")
        
        ai_output_path = Path(st.session_state.ai_output_folder)
        available_html_files = sorted([f.name for f in ai_output_path.iterdir() if f.is_file() and f.name.endswith('.html')])

        if not available_html_files:
            st.warning(f"No HTML files found in the output directory: {st.session_state.ai_output_folder}")
        else:
            query_params = st.query_params
            if "preview_file" in query_params:
                navigated_file = query_params.get("preview_file")
                if isinstance(navigated_file, list): navigated_file = navigated_file[0]
                if navigated_file in available_html_files and navigated_file != st.session_state.current_preview_file:
                    st.session_state.current_preview_file = navigated_file
            
            if not st.session_state.current_preview_file or st.session_state.current_preview_file not in available_html_files:
                st.session_state.current_preview_file = "index.html" if "index.html" in available_html_files else available_html_files[0]
            
            selected_file_for_preview = st.selectbox(
                "Select page to preview:",
                options=available_html_files,
                index=available_html_files.index(st.session_state.current_preview_file) if st.session_state.current_preview_file in available_html_files else 0,
                key="file_selector_dropdown_preview"
            )
            if selected_file_for_preview != st.session_state.current_preview_file:
                st.session_state.current_preview_file = selected_file_for_preview
                st.query_params["preview_file"] = selected_file_for_preview 
                st.rerun()

            if st.session_state.current_preview_file:
                display_preview(st.session_state.current_preview_file, st.session_state.ai_output_folder)

    st.markdown('</div>', unsafe_allow_html=True)
</file>

<file path="README.md">
# üöÄ AI Website Modernizer

**Transform any website into a modern, responsive, and SEO-optimized experience using Google Gemini AI.**

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red.svg)](https://streamlit.io/)

## üéØ Overview

AI Website Modernizer is an intelligent web transformation tool that analyzes existing websites and automatically rebuilds them with modern design principles, improved user experience, and optimized performance. Using advanced AI capabilities, it creates cohesive, responsive websites that maintain brand identity while dramatically improving functionality.

---

## ‚ú® Key Features

### ü§ñ **Intelligent Website Analysis**
- **Smart Content Extraction**: Automatically crawls and analyzes website structure, content, and design patterns
- **Comprehensive Data Collection**: Captures HTML, CSS, images, and copy with configurable depth and page limits
- **Content Understanding**: AI processes all page data simultaneously for holistic redesign approach

### üé® **AI-Powered Modernization**
- **Holistic Redesign**: Google Gemini AI analyzes entire site structure to create cohesive, modern designs
- **Mobile-First Approach**: Automatically optimizes for responsive design and mobile user experience
- **SEO Optimization**: Implements modern SEO best practices and performance improvements
- **Brand Consistency**: Maintains brand identity while upgrading design language and user experience

### üîß **Technical Excellence**
- **Structured Output**: Generates clean, maintainable code with global stylesheets and organized HTML
- **Scalable Architecture**: Three-stage pipeline (Crawl ‚Üí AI Processing ‚Üí Generation) for reliable results
- **Security-First**: Secure API key handling and path traversal protection
- **Error Resilience**: Comprehensive timeout handling and graceful error recovery

### üìä **User-Friendly Interface**
- **Interactive Dashboard**: Streamlit-powered UI for easy website transformation
- **Real-Time Monitoring**: Live progress tracking and detailed logging
- **Preview Capability**: Instant preview of transformed pages
- **Process Transparency**: Clear status indicators and detailed feedback

## üîÑ How It Works

```mermaid
graph LR
    A[Input Website URL] --> B[Web Crawling]
    B --> C[Content Extraction]
    C --> D[AI Analysis]
    D --> E[Design Generation]
    E --> F[Modern Website Output]
```

1. **üï∑Ô∏è Web Crawling**: Automatically discovers and crawls website pages using Selenium WebDriver
2. **üìù Content Extraction**: Extracts and structures HTML, CSS, images, and text content
3. **üß† AI Analysis**: Google Gemini processes all content simultaneously for comprehensive understanding
4. **üé® Design Generation**: AI creates modern, responsive design with improved UX and SEO
5. **üì¶ Output Generation**: Produces clean, organized website files ready for deployment

---

## üìã Prerequisites
- Python 3.9+
- Chrome browser and ChromeDriver (for Selenium)
- Google Gemini API key ([Get one here](https://ai.google.dev/gemini-api/docs/quickstart?lang=python))

## üöÄ Quick Start

### 1. **Clone & Setup**
```bash
git clone <repository-url>
cd ai-website-modernizer
```

### 2. **Environment Setup**
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Optional: Enhanced performance
pip install watchdog
```

### 3. **API Configuration**
```bash
# Set your Google Gemini API key
export GOOGLE_GEMINI_API_KEY=your-api-key-here

# For permanent setup (recommended)
echo 'export GOOGLE_GEMINI_API_KEY=your-api-key-here' >> ~/.bashrc
source ~/.bashrc
```

> üîë **Get your API key**: [Google AI Studio](https://ai.google.dev/gemini-api/docs/quickstart?lang=python)

### 4. **Launch Dashboard**
```bash
streamlit run dashboard.py
```

Open your browser to `http://localhost:8501` and start transforming websites! üéâ

---

## üíª Usage Options

### üéØ **Recommended: Interactive Dashboard**
```bash
streamlit run dashboard.py
```
- **User-friendly interface** with real-time progress tracking
- **One-click transformation** from URL input to modernized website
- **Live preview** of transformed pages
- **Detailed logging** and status monitoring

### üîß **Advanced: Command Line Interface**

#### Crawl Website
```bash
python crawl_site.py https://example.com --max_pages 10 --depth 2
```

#### AI Transformation
```bash
python remake_site_with_ai.py example.com --model gemini-2.5-flash-preview-05-20
```

**Available Models:**
- `gemini-2.5-flash-preview-05-20` (default, fast)
- `gemini-1.5-pro-latest` (comprehensive, large context)

## üìÅ Output Structure

```
üì¶ example.com/                    # Original crawled data
 ‚î£ üìÇ home/
 ‚îÉ ‚î£ üìÑ url.txt                   # Page URL
 ‚îÉ ‚î£ üìÑ page.html                 # Original HTML
 ‚îÉ ‚î£ üìÑ copy.txt                  # Extracted text content
 ‚îÉ ‚î£ üìÑ images.txt                # Image URLs
 ‚îÉ ‚îó üìÑ css.txt                   # CSS styles
 ‚î£ üìÇ about/
 ‚îÉ ‚îó üìÑ ...
 ‚îó üìÑ crawl_manifest.json         # Crawl metadata

üì¶ example.com_ai/                 # üé® AI-Enhanced Website
 ‚î£ üìÑ global_styles.css           # Modern global stylesheet
 ‚î£ üìÇ home/
 ‚îÉ ‚î£ üìÑ index.html                # ‚ú® Modernized HTML
 ‚îÉ ‚îó üìÑ original_*.txt            # Reference files
 ‚îó üìÇ about/
   ‚îó üìÑ index.html
```

## üèóÔ∏è Architecture

### **Three-Stage Pipeline**
```
üï∑Ô∏è Crawl ‚Üí üß† AI Processing ‚Üí üì¶ Generation
```

1. **Crawler Module** (`crawl_site.py`)
   - Selenium WebDriver for JavaScript-rendered sites
   - Structured content extraction and validation
   - Configurable depth and page limits

2. **AI Processing** (`remake_site_with_ai.py`)
   - Google Gemini integration with security-first design
   - Holistic content analysis and design generation
   - Enhanced path traversal protection

3. **Dashboard Interface** (`dashboard.py`)
   - Streamlit-powered user interface
   - Real-time progress monitoring
   - Manifest-based reliable inter-process communication

## üß† AI & Technical Details

### **Prompt Engineering**
- **Holistic Analysis**: All crawled content processed simultaneously for cohesive design
- **Design Principles**: Mobile-first responsiveness, SEO optimization, accessibility
- **Brand Preservation**: Maintains original brand identity while modernizing experience
- **Structured Output**: JSON-formatted response with global CSS and individual page HTML

### **Security & Reliability**
- **üîí Secure API Handling**: Environment-based API key management
- **üõ°Ô∏è Path Traversal Protection**: Enhanced filename validation for AI-generated content
- **‚è±Ô∏è Timeout Management**: Prevents hanging on unresponsive pages
- **üìä Manifest Communication**: Reliable inter-script communication replacing brittle stdout parsing

### **Performance Considerations**
- **Context Window**: Leverages large context windows (1M+ tokens) for comprehensive analysis
- **Scalable Architecture**: Modular design supports easy enhancement and maintenance
- **Error Resilience**: Graceful handling of network issues, API limits, and malformed content

## ü§ù Contributing

We welcome contributions! Here's how you can help:

### **Areas for Enhancement**
- üåê **Additional AI Models**: Support for Claude, GPT-4, or other language models
- üöÄ **Performance Optimization**: Parallel processing for large sites
- üé® **Design Templates**: Pre-built design themes and templates
- üì± **Mobile Optimization**: Enhanced mobile-specific optimizations
- üîç **SEO Features**: Advanced SEO analysis and recommendations

### **Development Setup**
```bash
# Fork the repository and clone your fork
git clone https://github.com/your-username/ai-website-modernizer.git

# Create feature branch
git checkout -b feature/your-feature-name

# Make changes and test thoroughly
python -m pytest tests/  # When tests are added

# Submit pull request
```

## üìà Roadmap

- [ ] **Multi-Model Support**: Integration with Claude, GPT-4, and other AI models
- [ ] **Template System**: Pre-built modern design templates
- [ ] **Batch Processing**: Support for multiple websites simultaneously
- [ ] **Performance Analytics**: Before/after performance comparison
- [ ] **Custom Branding**: Advanced brand guideline integration
- [ ] **API Development**: RESTful API for programmatic access

---

## üõ†Ô∏è Troubleshooting

### **Common Issues & Solutions**

| Issue | Solution |
|-------|----------|
| **Missing Dependencies** | Run `pip install -r requirements.txt` in activated virtual environment |
| **ChromeDriver Issues** | Ensure ChromeDriver matches Chrome version and is in PATH |
| **API Key Not Found** | Verify `GOOGLE_GEMINI_API_KEY` environment variable is set |
| **Timeout Errors** | Check internet connection; large sites may need multiple attempts |
| **JSON Parse Errors** | AI model occasionally returns malformed JSON; retry the process |
| **Memory Issues** | Use `gemini-2.5-flash-preview-05-20` for large sites instead of pro model |

### **Debug Mode**
```bash
# Enable verbose logging
export DEBUG_MODE=1
streamlit run dashboard.py
```

### **Getting Help**
- üìñ **Documentation**: Check function docstrings and comments
- üêõ **Issues**: Report bugs via GitHub Issues
- üí¨ **Discussions**: Join community discussions for usage questions
- üìß **Contact**: Reach out for enterprise or custom solutions

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---
</file>

<file path="remake_site_with_ai.py">
import os
import sys
from termcolor import cprint
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import argparse
import shutil
import json

# --- File name constants ---
COPY_FILENAME = "copy.txt"
CSS_FILENAME = "css.txt"
HTML_FILENAME = "page.html"
IMAGES_FILENAME = "images.txt"
URL_FILENAME = "url.txt"

# --- Gemini API helpers ---
def load_gemini_api_key():
    cprint("[INFO] Loading Gemini API key...", "cyan")
    api_key = os.getenv("GOOGLE_GEMINI_API_KEY")
    if api_key:
        cprint("[SUCCESS] API key loaded from environment variable", "green")
        return api_key
    else:
        cprint("[ERROR] GOOGLE_GEMINI_API_KEY environment variable not set!", "red")
        cprint("[INFO] Please set your API key: export GOOGLE_GEMINI_API_KEY=your-api-key", "yellow")
        return None

def gemini_generate_entire_site(all_pages_data_str, model_name="gemini-2.5-flash-preview-05-20", temperature=0.5):
    cprint(f"[INFO] Initializing Gemini API for site generation...", "cyan")
    api_key = load_gemini_api_key()
    if not api_key:
        return None
    
    prompt = f"""
You are an expert web development agency tasked with a complete website overhaul and reimagining.
You will receive data from an existing website, including HTML, CSS, text copy, and image URLs for multiple pages.

Your goal is to **analyze this content and rebuild it into the best possible modern website structure**.
You have full autonomy to decide whether a single-page website or a multi-page website would be most effective for this content.

**Key Requirements:**

1.  **Structural Decision Making:** 
    *   Analyze the amount, type, and relationships of the provided content.
    *   Decide whether a single-page scrolling website or a multi-page website would better serve the content and user experience.
    *   Consider factors like: content volume, distinct topic areas, navigation complexity, and modern web best practices.

2.  **Design Excellence & Reusable Components:**
    *   Create a modern, visually appealing, **highly responsive (mobile-first)**, and **SEO-optimized** website.
    *   Ensure consistent design language, branding (colors, fonts, imagery based on original assets), and user experience.
    *   **CRITICAL: Design with reusable components in mind.** Think about common elements like headers, footers, navigation menus, hero sections, call-to-action buttons, service cards, testimonial blocks, etc. 
    *   Apply these conceptual components consistently across the generated page(s) to ensure a cohesive and professional look and feel. While you will output full HTML for each page, your internal design process should emphasize this component-based thinking for consistency.

3.  **Content Modernization:** 
    *   Improve the original copy for clarity, engagement, and conciseness while retaining the core message.
    *   Reorganize content logically based on your structural decision.
    *   Ensure proper information hierarchy and flow.

4.  **Technical Excellence:**
    *   Use semantic HTML5 tags (e.g., `<header>`, `<footer>`, `<nav>`, `<main>`, `<article>`, `<aside>`, `<section>`).
    *   Ensure proper heading hierarchy (H1, H2, H3, etc.).
    *   Generate descriptive meta tags (title, description) for each page, derived from content.
    *   Optimize images with appropriate `alt` text.
    *   Utilize the provided image URLs from the original site directly in `<img>` tags.

5.  **Navigation & Linking:**
    *   For single-page sites: Implement smooth scrolling navigation to sections (e.g., using anchor links like `<a href="#about-section">`).
    *   For multi-page sites: Ensure consistent navigation with working links between pages.
    *   Include appropriate call-to-action buttons and internal links.
    *   Make logos/brand names link to the home/top of the site (i.e., to "index.html" or "#top" for a one-pager).

6.  **Output Format - CRITICAL:**
    You MUST output a single JSON object with these exact top-level keys:

    *   `"site_structure_decision"`: A brief string explaining your structural choice and reasoning (e.g., "Single-page website chosen for concise content and better user flow. Reusable header/footer components applied.", "Multi-page site with Home, About, Services chosen to properly organize substantial content areas. Consistent header, footer, and navigation components implemented across all pages.").

    *   `"global_css"`: A string containing all CSS rules for the entire website. This will be saved as `global_styles.css`.

    *   `"html_files"`: An object where:
        - Each key is a filename (e.g., "index.html", "about.html", "services.html"). These filenames will be used directly.
        - Each value is the complete HTML content for that file (as a string).
        - For single-page sites: Only "index.html" should be present as a key in this object.
        - For multi-page sites: The main landing page MUST be keyed as "index.html". Additional pages should have simple, descriptive filenames (e.g., "about.html", "contact.html").
        - All HTML files will be saved in the same root directory alongside `global_styles.css`.
        - Links *between* generated HTML pages (if any) should use simple relative paths (e.g., `<a href="about.html">`, `<a href="services.html">`).
        - All HTML files must link to the global CSS file using a simple relative path, like `<link rel="stylesheet" href="global_styles.css">`.
        - Each HTML file should be a complete document starting with `<!DOCTYPE html>`.

7.  **Content Integration Guidelines:**
    *   Use ALL provided content strategically - don't discard valuable information.
    *   If creating a single-page site, organize content into logical sections with clear headings and corresponding navigation.
    *   If creating multi-page site, distribute content meaningfully across pages.
    *   Maintain the essence and value propositions from the original site.
    *   Ensure contact information, services, and key messaging are prominently featured.

**Input Data (Original Website Structure):**
The following contains data from all crawled pages. Use this content to make your structural decisions and build the new site:

<website_data>
{all_pages_data_str}
</website_data>

---
**IMPORTANT**: Respond ONLY with the JSON object as specified above. Do not include any other text, explanations, or markdown formatting around the JSON.
---
"""
    
    data_size_kb = len(all_pages_data_str) / 1024
    cprint(f"[INFO] Preparing to send {data_size_kb:.1f}KB of site data to Gemini", "cyan")
    cprint(f"[INFO] Using model: {model_name}", "cyan")
    cprint(f"[INFO] This may take several minutes for large sites...", "yellow")
    
    response_text = None
    try:
        cprint("[INFO] Configuring Gemini API...", "cyan")
        genai.configure(api_key=api_key)
        model_instance = genai.GenerativeModel(
            model_name=model_name,
            # safety_settings allow all content - use with caution and ensure your use case complies with policies
            # safety_settings={
            #     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            #     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            #     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            #     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            # }
        )
        cprint("[SUCCESS] Gemini API configured successfully", "green")
        
        cprint("[INFO] Sending request to Gemini AI... ‚è≥", "cyan", attrs=["bold"])
        response = model_instance.generate_content(
            contents=prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=temperature, 
                # max_output_tokens=8192, # Explicitly set if needed, flash default is 8192. Pro might be higher.
            )
        )
        cprint("[SUCCESS] Received response from Gemini AI! üéâ", "green", attrs=["bold"])
        
        cprint("[INFO] Processing AI response...", "cyan")

        # Try to access text, handle potential errors
        try:
            if response.parts:
                response_text = "".join(part.text for part in response.parts if hasattr(part, 'text'))
            else: 
                response_text = response.text 
        except Exception as e:
            cprint(f"[WARN] Could not access response.text or response.parts directly: {e}", "yellow")
            cprint(f"[DEBUG] Full response object: {response}", "yellow")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                cprint(f"[ERROR] Prompt blocked due to: {response.prompt_feedback.block_reason}", "red")
                cprint(f"[DEBUG] Safety ratings: {response.prompt_feedback.safety_ratings}", "yellow")
            if response.candidates and response.candidates[0].finish_reason:
                 cprint(f"[ERROR] Generation candidate finished due to: {response.candidates[0].finish_reason}", "red")
                 if hasattr(response.candidates[0].finish_reason, 'name') and response.candidates[0].finish_reason.name == "MAX_TOKENS":
                     cprint("[HINT] The model may have run out of output tokens. Consider a model with larger output capacity or reducing prompt/output complexity.", "yellow")
                 elif hasattr(response.candidates[0].finish_reason, 'name') and response.candidates[0].finish_reason.name == "SAFETY":
                     cprint("[HINT] Content generation stopped due to safety settings. Review safety ratings above.", "yellow")
            return None

        if not response_text:
            cprint("[ERROR] Gemini response was empty or contained no text parts.", "red")
            cprint(f"[DEBUG] Full response object: {response}", "yellow")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                cprint(f"[ERROR] Prompt blocked due to: {response.prompt_feedback.block_reason}", "red")
            if response.candidates and response.candidates[0].finish_reason:
                 cprint(f"[ERROR] Generation candidate finished due to: {response.candidates[0].finish_reason}", "red")
            return None

        cleaned_text = response_text.strip()
        
        # Clean markdown formatting if present
        if cleaned_text.startswith("```json"):
            cprint("[INFO] Removing ```json markdown wrapper", "cyan")
            cleaned_text = cleaned_text[len("```json"):] 
        elif cleaned_text.startswith("```"): 
            cprint("[INFO] Removing ``` markdown wrapper", "cyan")
            cleaned_text = cleaned_text[len("```"):]
        if cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-len("```")]
        
        cprint("[INFO] Parsing JSON response...", "cyan")
        ai_json_response = json.loads(cleaned_text)
        
        # Validate response structure
        if not isinstance(ai_json_response, dict):
            raise ValueError("Response is not a JSON object")
        if "site_structure_decision" not in ai_json_response:
            raise ValueError("Missing 'site_structure_decision' key in response")
        if "global_css" not in ai_json_response:
            raise ValueError("Missing 'global_css' key in response")
        if "html_files" not in ai_json_response:
            raise ValueError("Missing 'html_files' key in response")
        if not isinstance(ai_json_response["html_files"], dict):
            raise ValueError("'html_files' value is not an object")
            
        pages_count = len(ai_json_response["html_files"])
        css_size_kb = len(ai_json_response["global_css"]) / 1024
        cprint(f"[SUCCESS] Parsed AI response: {pages_count} pages, {css_size_kb:.1f}KB CSS", "green")
        
        return ai_json_response
        
    except json.JSONDecodeError as e:
        cprint(f"[ERROR] Failed to parse JSON response from Gemini: {e}", "red")
        cprint(f"[DEBUG] Raw text before JSON parse (first 500 chars):", "yellow")
        if response_text:
            cprint(f">>>>\n{response_text[:500]}\n<<<<", "yellow")
        else:
            cprint("[DEBUG] No text was extracted from the response to parse.", "yellow")
        return None
    except Exception as e:
        cprint(f"[ERROR] Gemini API interaction or processing failed: {e}", "red")
        import traceback
        traceback.print_exc()
        # More detailed debugging for the response object if available
        if 'response' in locals():
            cprint(f"[DEBUG] Full response object at time of error: {response}", "yellow")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
               cprint(f"[ERROR] Prompt blocked due to: {response.prompt_feedback.block_reason}", "red")
            if response.candidates:
                for i, cand in enumerate(response.candidates):
                    cprint(f"[DEBUG] Candidate {i} Finish Reason: {cand.finish_reason}", "yellow")
                    cprint(f"[DEBUG] Candidate {i} Safety Ratings: {cand.safety_ratings}", "yellow")
        return None

def main():
    parser = argparse.ArgumentParser(description="Rebuild an entire crawled website using AI with a holistic approach.")
    parser.add_argument("site_folder", help="The folder containing the crawled site data (e.g., example.com).")
    parser.add_argument("--model", default="gemini-2.5-flash-preview-05-20",
                        help="Name of the Gemini model to use (e.g., gemini-2.5-flash-preview-05-20, gemini-1.5-pro-latest).")
    parser.add_argument("--temperature", type=float, default=0.5,
                        help="Temperature for AI generation (0.0-1.0, higher values make output more creative/random).")
    
    args = parser.parse_args()

    site_folder = args.site_folder
    
    cprint("="*70, "cyan")
    cprint("ü§ñ AI WEBSITE REBUILDER STARTING", "cyan", attrs=["bold"])
    cprint("="*70, "cyan")
    cprint(f"[INFO] Source folder: {site_folder}", "cyan")
    cprint(f"[INFO] AI Model: {args.model}", "cyan")
    cprint("="*70, "cyan")

    # Validate input folder
    if not os.path.isdir(site_folder):
        cprint(f"[ERROR] {site_folder} is not a directory!", "red")
        sys.exit(1)

    # Set up output folder
    out_folder = site_folder.rstrip('/').rstrip('\\') + "_ai"
    cprint(f"[INFO] Output folder will be: {out_folder}", "cyan")
    
    if os.path.exists(out_folder):
        cprint(f"[WARN] Output folder {out_folder} already exists, will overwrite files", "magenta")
    
    try:
        os.makedirs(out_folder, exist_ok=True)
        cprint(f"[SUCCESS] Output directory ready: {out_folder}", "green")
    except Exception as e:
        cprint(f"[ERROR] Failed to create output directory: {e}", "red")
        sys.exit(1)

    # Scan for page subdirectories
    cprint(f"[INFO] Scanning for page data in {site_folder}...", "cyan")
    try:
        page_subdirs = [f.name for f in os.scandir(site_folder) if f.is_dir()]
        cprint(f"[SUCCESS] Found {len(page_subdirs)} page directories: {', '.join(page_subdirs) if page_subdirs else 'None'}", "green")
    except Exception as e:
        cprint(f"[ERROR] Failed to scan site folder: {e}", "red")
        sys.exit(1)

    if not page_subdirs:
        cprint(f"[ERROR] No page subdirectories found in {site_folder}. Ensure crawl_site.py ran successfully.", "red")
        sys.exit(1)

    # Load page data
    all_pages_data = []
    successful_pages_loaded = 0
    
    cprint("\n" + "="*50, "cyan")
    cprint("üìÑ LOADING PAGE DATA", "cyan", attrs=["bold"])
    cprint("="*50, "cyan")

    for page_subdir_name in page_subdirs:
        page_dir = os.path.join(site_folder, page_subdir_name)
        cprint(f"[INFO] Processing page data from directory: {page_subdir_name}", "cyan")
        
        # Define expected files
        copy_path = os.path.join(page_dir, COPY_FILENAME)
        css_path = os.path.join(page_dir, CSS_FILENAME)
        html_path = os.path.join(page_dir, HTML_FILENAME)
        images_path = os.path.join(page_dir, IMAGES_FILENAME)
        url_path = os.path.join(page_dir, URL_FILENAME)

        # Check for essential files
        essential_files = [copy_path, html_path, url_path]
        missing_files = [f for f in essential_files if not os.path.exists(f)]
        
        if missing_files:
            cprint(f"  [WARN] Skipping {page_subdir_name} - missing essential files: {[os.path.basename(f) for f in missing_files]}", "magenta")
            continue

        try:
            # Load all page data
            with open(url_path, "r", encoding="utf-8") as f: 
                page_url = f.read().strip()
                cprint(f"    ‚úì Loaded URL: {page_url}", "green")
                
            with open(html_path, "r", encoding="utf-8") as f: 
                original_html = f.read()
                html_size_kb = len(original_html) / 1024
                cprint(f"    ‚úì Loaded HTML: {html_size_kb:.1f}KB", "green")
                
            with open(copy_path, "r", encoding="utf-8") as f: 
                original_copy = f.read()
                word_count = len(original_copy.split())
                cprint(f"    ‚úì Loaded copy: {word_count} words", "green")
            
            # Load optional files
            original_css = "/* No external CSS file found or it was empty. */"
            if os.path.exists(css_path):
                with open(css_path, "r", encoding="utf-8") as f: 
                    css_content = f.read().strip()
                    if css_content:
                        original_css = css_content
                        cprint(f"    ‚úì Loaded CSS: {len(css_content)} chars", "green")
                    else:
                        cprint(f"    ‚ö† CSS file empty for {page_subdir_name}", "yellow")
            else:
                cprint(f"    ‚ö† No CSS file (css.txt) found for {page_subdir_name}", "yellow")
            
            image_urls = "<!-- No image URLs provided or images.txt was empty. -->"
            if os.path.exists(images_path):
                with open(images_path, "r", encoding="utf-8") as f: 
                    content = f.read().strip()
                    if content:
                        image_urls = content
                        image_count = len(content.split('\n'))
                        cprint(f"    ‚úì Loaded images: {image_count} URLs", "green")
                    else:
                        cprint(f"    ‚ö† Images file empty for {page_subdir_name}", "yellow")
            else:
                cprint(f"    ‚ö† No images file (images.txt) found for {page_subdir_name}", "yellow")

            # Create XML data block
            page_data_xml = f"""  <page id="{page_subdir_name}">
    <url><![CDATA[{page_url}]]></url>
    <original_html><![CDATA[{original_html}]]></original_html>
    <original_css><![CDATA[{original_css}]]></original_css>
    <original_copy><![CDATA[{original_copy}]]></original_copy>
    <image_urls><![CDATA[{image_urls}]]></image_urls>
  </page>"""
            all_pages_data.append(page_data_xml)
            successful_pages_loaded += 1
            cprint(f"  [SUCCESS] Successfully processed data for page: {page_subdir_name}", "green", attrs=["bold"])
            
        except Exception as e:
            cprint(f"  [ERROR] Failed to load data for {page_subdir_name}: {e}", "red")

    if not all_pages_data:
        cprint(f"\n[ERROR] No valid page data could be loaded after processing all subdirectories. AI processing cannot continue.", "red")
        sys.exit(1)
    
    cprint(f"\n[SUCCESS] Successfully loaded data for {successful_pages_loaded} pages for AI processing", "green", attrs=["bold"])
    
    # Prepare data for AI
    all_pages_data_str = "\n".join(all_pages_data)
    
    cprint("\n" + "="*50, "cyan")
    cprint("ü§ñ AI PROCESSING", "cyan", attrs=["bold"])
    cprint("="*50, "cyan")
    
    # Send to AI
    ai_response = gemini_generate_entire_site(all_pages_data_str, model_name=args.model, temperature=args.temperature)

    if not ai_response:
        cprint("\n[ERROR] AI processing failed - no valid response received from Gemini.", "red")
        sys.exit(1)

    # Validate AI response structure (already did in gemini_generate_entire_site, but good for safety)
    if not ("site_structure_decision" in ai_response and "global_css" in ai_response and "html_files" in ai_response and isinstance(ai_response["html_files"], dict)):
        cprint(f"\n[ERROR] Invalid AI response structure after receiving from Gemini.", "red")
        cprint(f"[DEBUG] Response keys: {list(ai_response.keys()) if isinstance(ai_response, dict) else 'Not a dict'}", "yellow")
        sys.exit(1)

    cprint("\n" + "="*50, "green")
    cprint("üíæ SAVING AI-GENERATED WEBSITE", "green", attrs=["bold"])
    cprint("="*50, "green")

    # Display AI's structural decision
    if "site_structure_decision" in ai_response:
        cprint(f"[AI DECISION] {ai_response['site_structure_decision']}", "magenta", attrs=["bold"])
    
    # Save AI decision for dashboard display
    decision_path = os.path.join(out_folder, "ai_decision.txt")
    try:
        with open(decision_path, "w", encoding="utf-8") as f:
            f.write(ai_response['site_structure_decision'])
        cprint(f"[INFO] AI decision saved to {decision_path}", "cyan")
    except Exception as e:
        cprint(f"[WARN] Failed to save AI decision: {e}", "yellow")
    
    # Save global CSS
    cprint("[INFO] Saving global stylesheet...", "cyan")
    try:
        global_css_path = os.path.join(out_folder, "global_styles.css")
        with open(global_css_path, "w", encoding="utf-8") as f:
            f.write(ai_response["global_css"])
        css_size_kb = len(ai_response["global_css"]) / 1024
        cprint(f"  [SUCCESS] Saved global CSS: {global_css_path} ({css_size_kb:.1f}KB)", "green", attrs=["bold"])
    except Exception as e:
        cprint(f"  [ERROR] Failed to save global CSS: {e}", "red")
        sys.exit(1) # Critical failure if CSS can't be saved

    # Save HTML files directly in root output directory
    saved_page_count = 0
    failed_page_save_count = 0
    
    for filename, page_html_content in ai_response["html_files"].items():
        cprint(f"[INFO] Saving HTML file: {filename}", "cyan")
        
        # Enhanced security validation for filename
        if not filename.endswith(".html"):
            cprint(f"  [WARN] Skipping non-HTML filename from AI: {filename}", "magenta")
            failed_page_save_count += 1
            continue
        
        # Normalize and validate path to prevent directory traversal
        normalized_filename = os.path.normpath(filename)
        if (normalized_filename != filename or 
            ".." in normalized_filename or 
            "/" in normalized_filename or "\\" in normalized_filename or
            normalized_filename.startswith('.') or
            any(ord(c) < 32 for c in normalized_filename)):  # Check for control characters
            cprint(f"  [WARN] Skipping potentially unsafe filename from AI: {filename}", "magenta")
            failed_page_save_count += 1
            continue
            
        if not isinstance(page_html_content, str):
            cprint(f"  [WARN] HTML content for '{filename}' is not a string (type: {type(page_html_content)}), skipping.", "magenta")
            failed_page_save_count += 1
            continue

        # Validate HTML content with BeautifulSoup
        try:
            from bs4 import BeautifulSoup
            parsed_html = BeautifulSoup(page_html_content, "html.parser")
            # Basic validation: check if parsing succeeded and content is reasonable
            if not parsed_html or len(str(parsed_html).strip()) < 50:
                cprint(f"  [WARN] AI-generated HTML for '{filename}' appears to be malformed or too short.", "magenta")
                cprint(f"        Content preview: {page_html_content[:100]}...", "yellow")
                # Continue with saving anyway, but log the warning
            else:
                cprint(f"  ‚úì HTML validation passed for {filename}", "green")
        except Exception as e_validation:
            cprint(f"  [WARN] HTML validation failed for '{filename}': {e_validation}", "magenta")
            cprint(f"        Content preview: {page_html_content[:100]}...", "yellow")
            # Continue with saving anyway since validation failure doesn't mean unusable content

        try:
            # Save HTML file directly in root output directory
            output_html_path = os.path.join(out_folder, filename)
            with open(output_html_path, "w", encoding="utf-8") as f:
                f.write(page_html_content)
            
            html_size_kb = len(page_html_content) / 1024
            cprint(f"    ‚úì Saved {filename} ({html_size_kb:.1f}KB)", "green")
            saved_page_count += 1
            
        except Exception as e_save:
            cprint(f"  [ERROR] Failed to save {filename}: {e_save}", "red")
            failed_page_save_count += 1

    # Copy original crawled site data to a subfolder for reference
    cprint("[INFO] Preserving original crawled data for reference...", "cyan")
    try:
        originals_ref_dir = os.path.join(out_folder, "original_crawled_data")
        if os.path.exists(site_folder):
            shutil.copytree(site_folder, originals_ref_dir, dirs_exist_ok=True)
            cprint(f"  ‚úì Original data preserved in: {originals_ref_dir}", "green")
        else:
            cprint(f"  [WARN] Source folder {site_folder} not found for reference copy", "yellow")
    except Exception as e_copy:
        cprint(f"  [WARN] Could not copy original crawled data: {e_copy}", "yellow")

    # Final summary
    cprint("\n" + "="*70, "green")
    cprint("üéâ AI WEBSITE REBUILD COMPLETED", "green", attrs=["bold"])
    cprint("="*70, "green")
    
    if saved_page_count > 0:
        cprint(f"[SUCCESS] {saved_page_count} HTML files were successfully generated and saved.", "green", attrs=["bold"])
        cprint(f"[SUCCESS] Output directory: {out_folder}", "green", attrs=["bold"])
        
        if failed_page_save_count > 0:
            cprint(f"[WARN] {failed_page_save_count} files encountered issues during saving.", "yellow")
        
        cprint("\nüìÅ Generated files include:", "cyan")
        cprint(f"  ‚Ä¢ global_styles.css (Global stylesheet)", "cyan")
        
        # List saved HTML files
        successfully_saved_files = [filename for filename in ai_response["html_files"].keys() 
                                  if isinstance(ai_response["html_files"][filename], str) 
                                  and filename.endswith(".html") 
                                  and "/" not in filename and "\\" not in filename][:5]
        
        for filename in successfully_saved_files:
            cprint(f"  ‚Ä¢ {filename} (Ready-to-deploy HTML page)", "cyan")
        if len(ai_response["html_files"]) > 5:
            cprint(f"  ‚Ä¢ ... and {len(ai_response['html_files']) - 5} more HTML file(s).", "cyan")
            
        cprint(f"  ‚Ä¢ original_crawled_data/ (Reference folder with original site data)", "cyan")
        
        # Check for index.html
        if "index.html" in successfully_saved_files:
            cprint(f"\nüöÄ DEPLOYMENT READY: Your site has an index.html and is ready to deploy!", "green", attrs=["bold"])
            cprint(f"   Deploy to Vercel: Run 'vercel' in the {out_folder} directory", "cyan")
            cprint(f"   Or open {out_folder}/index.html in your browser to preview locally", "cyan")
        else:
            cprint(f"\n‚ö†Ô∏è  Note: No index.html found. You may need to manually designate a main page for deployment.", "yellow")
        
    else:
        cprint(f"[ERROR] No HTML files were successfully saved from the AI response.", "red")
        if failed_page_save_count > 0:
            cprint(f"[INFO] {failed_page_save_count} files encountered issues during the saving process.", "yellow")
        sys.exit(1) # Indicate failure if no files saved

    cprint("="*70, "green")

if __name__ == "__main__":
    main()
</file>

</files>
